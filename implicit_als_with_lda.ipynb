{
 "metadata": {
  "name": "",
  "signature": "sha256:a7e2191690d73bba0ce3fdaff59562dccd692bc1906eb5a55a28dd5b659fa463"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "dataset = pd.read_csv(\"./events.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset[\"like\"] = dataset[\"like\"] == 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset[dataset[\"like\"]].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset[~dataset[\"like\"]].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_duplicates(dataset):\n",
      "    return dataset.loc[dataset[[\"user_id\", \"post_permlink\"]].drop_duplicates().index]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = remove_duplicates(dataset).reset_index()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def threshold_likes(df, uid_min, mid_min):\n",
      "    n_users = df.user_id.unique().shape[0]\n",
      "    n_items = df.post_permlink.unique().shape[0]\n",
      "    sparsity = float(df.shape[0]) / float(n_users*n_items) * 100\n",
      "    print('Starting likes info')\n",
      "    print('Number of users: {}'.format(n_users))\n",
      "    print('Number of models: {}'.format(n_items))\n",
      "    print('Sparsity: {:4.3f}%'.format(sparsity))\n",
      "    \n",
      "    done = False\n",
      "    while not done:\n",
      "        starting_shape = df.shape[0]\n",
      "        mid_counts = df.groupby('user_id').post_permlink.count()\n",
      "        df = df[~df.user_id.isin(mid_counts[mid_counts < mid_min].index.tolist())]\n",
      "        uid_counts = df.groupby('post_permlink').user_id.count()\n",
      "        df = df[~df.post_permlink.isin(uid_counts[uid_counts < uid_min].index.tolist())]\n",
      "        ending_shape = df.shape[0]\n",
      "        if starting_shape == ending_shape:\n",
      "            done = True\n",
      "    \n",
      "    assert(df.groupby('user_id').post_permlink.count().min() >= mid_min)\n",
      "    assert(df.groupby('post_permlink').user_id.count().min() >= uid_min)\n",
      "    \n",
      "    n_users = df.user_id.unique().shape[0]\n",
      "    n_items = df.post_permlink.unique().shape[0]\n",
      "    sparsity = float(df.shape[0]) / float(n_users*n_items) * 100\n",
      "    print('Ending likes info')\n",
      "    print('Number of users: {}'.format(n_users))\n",
      "    print('Number of models: {}'.format(n_items))\n",
      "    print('Sparsity: {:4.3f}%'.format(sparsity))\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = threshold_likes(dataset, 5, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = dataset.reset_index()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- \u0421\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0435\u043d\u0438\u0439\n",
      "- \u041f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u044c als\n",
      "- \u041f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**\u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0435\u043d\u0438\u0439**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def map_ids(row, mapper):\n",
      "    return mapper[row]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import sparse\n",
      "import numpy as np\n",
      "\n",
      "uid_to_idx = {}\n",
      "mid_to_idx = {}\n",
      "for (idx, uid) in enumerate(dataset[\"user_id\"].unique().tolist()):\n",
      "    uid_to_idx[uid] = idx\n",
      "    \n",
      "for (idx, mid) in enumerate(dataset[\"post_permlink\"].unique().tolist()):\n",
      "    mid_to_idx[mid] = idx\n",
      "    \n",
      "idx_to_uid = {v: k for k, v in uid_to_idx.items()}\n",
      "idx_to_mid = {v: k for k, v in mid_to_idx.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_matrix(tuples, train, alpha=1):\n",
      "    dataset = tuples.loc[train]\n",
      "    I = dataset[\"user_id\"].apply(map_ids, args=[uid_to_idx]).as_matrix()\n",
      "    J = dataset[\"post_permlink\"].apply(map_ids, args=[mid_to_idx]).as_matrix()\n",
      "    V = np.hstack(dataset[\"like\"].apply(lambda x: 1 if x else -1) * alpha)\n",
      "    shape = (tuples[\"user_id\"].unique().size, tuples[\"post_permlink\"].unique().size)\n",
      "    likes = sparse.coo_matrix((V, (I, J)), shape=shape, dtype=np.float64).tocsr()\n",
      "#     likes[likes < 0] = -1\n",
      "#     likes[likes > 0] = 1\n",
      "    return likes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e \u043c\u043e\u0434\u0435\u043b\u0438"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset[\"post_id\"] = dataset[\"post_permlink\"].apply(lambda x: mid_to_idx[x])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def average_precision_k(model, train_matrix, test, k):\n",
      "    precisions = []\n",
      "    for user in test[\"user_id\"].unique():\n",
      "        pred_items = [item for item, _ in model.recommend(uid_to_idx[user], train_matrix.T, N=k)]\n",
      "        true_items = test[(test[\"user_id\"] == user) & test[\"like\"]][\"post_id\"]\n",
      "        right_items = [x for x in true_items if x in pred_items]\n",
      "        if (true_items.size != 0):\n",
      "            precisions.append(float(len(right_items) / min(k, true_items.size)))\n",
      "    return np.mean(precisions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test for precision method (with 100% precision)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TestModel:\n",
      "    def recommend(self, user_id, matrix, N):\n",
      "        all_posts = dataset[(dataset[\"user_id\"] == idx_to_uid[user_id]) & (dataset[\"like\"])][\"post_id\"]\n",
      "        train_posts = matrix.T[user_id].indices\n",
      "        return [(post, 1) for post in all_posts if post not in train_posts][0:N]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_matrix = create_matrix(dataset, dataset.iloc[100:200].index).T\n",
      "model = TestModel()\n",
      "average_precision_k(model, train_matrix, dataset, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use cross validation by roc auc and precision k"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import ShuffleSplit\n",
      "from sklearn.metrics import roc_auc_score\n",
      "import implicit\n",
      "\n",
      "def cross_validate_roc_auc(n_splits, **kwargs):\n",
      "    train_scores = []\n",
      "    test_scores = []\n",
      "    generator = ShuffleSplit(n_splits=n_splits)\n",
      "    for train, test in generator.split(dataset):\n",
      "        train_dataset = dataset.iloc[train]\n",
      "        test_dataset = dataset.iloc[test]\n",
      "        train_matrix = create_matrix(dataset, train, alpha=40).T\n",
      "        model = implicit.als.AlternatingLeastSquares(**kwargs)\n",
      "        model.fit(train_matrix)\n",
      "        result_matrix = np.dot(model.user_factors, model.item_factors.T)\n",
      "        y_true = train_dataset[\"like\"].apply(lambda x: 1 if x > 0 else -1)\n",
      "        y_pred = [result_matrix[uid_to_idx[row[\"user_id\"]], mid_to_idx[row[\"post_permlink\"]]] for _, row in train_dataset.iterrows()]\n",
      "        train_scores.append(roc_auc_score(y_true, y_pred))\n",
      "        y_true = test_dataset[\"like\"].apply(lambda x: 1 if x > 0 else -1)\n",
      "        y_pred = [result_matrix[uid_to_idx[row[\"user_id\"]], mid_to_idx[row[\"post_permlink\"]]] for _, row in test_dataset.iterrows()]\n",
      "        test_scores.append(roc_auc_score(y_true, y_pred))\n",
      "    return train_scores, test_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import ShuffleSplit\n",
      "import implicit\n",
      "\n",
      "def cross_validate_precision_k(n_splits, k, **kwargs):\n",
      "    scores = []\n",
      "    generator = ShuffleSplit(n_splits=n_splits)\n",
      "    for train, test in generator.split(dataset):\n",
      "        train_dataset = dataset.iloc[train]\n",
      "        test_dataset = dataset.iloc[test]\n",
      "        train_matrix = create_matrix(dataset, train, alpha=40).T\n",
      "        print(\"Created matrix\")\n",
      "        model = implicit.als.AlternatingLeastSquares(**kwargs)\n",
      "        model.fit(train_matrix)\n",
      "        print(\"Prepared model\")\n",
      "        scores.append(average_precision_k(model, train_matrix, test_dataset, k))\n",
      "        print(\"Prepared score\")\n",
      "    return scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def annoy_predict(model, user, dataset, k):\n",
      "    result = []\n",
      "    for item in dataset[dataset[\"user_id\"] == user]:\n",
      "        result.append()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def annoy_average_precision_k(model, train_matrix, test, k):\n",
      "    precisions = []\n",
      "    for user in test[\"user_id\"].unique():\n",
      "        pred_items = annoy_predict(model, user, test, k)\n",
      "        true_items = test[(test[\"user_id\"] == user) & test[\"like\"]][\"post_id\"]\n",
      "        right_items = [x for x in true_items if x in pred_items]\n",
      "        if (true_items.size != 0):\n",
      "            precisions.append(float(len(right_items) / min(k, true_items.size)))\n",
      "    return np.mean(precisions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import ShuffleSplit\n",
      "from annoy import AnnoyIndex\n",
      "\n",
      "def annoy_cross_validate_precision_k(n_splits, k, n_trees):\n",
      "    scores = []\n",
      "    generator = ShuffleSplit(n_splits=n_splits)\n",
      "    for train, test in generator.split(dataset):\n",
      "        train_dataset = dataset.iloc[train]\n",
      "        test_dataset = dataset.iloc[test]\n",
      "        train_matrix = create_matrix(dataset, train, alpha=40).T\n",
      "        print(\"Created matrix\")\n",
      "        annoy = AnnoyIndex(train_matrix.shape[1])\n",
      "        for index, row in enumerate(train_matrix.T.todense()):\n",
      "            annoy.add_item(index, row)\n",
      "        annoy.build(n_trees)\n",
      "        print(\"Prepared model\")\n",
      "        scores.append(average_precision_k(model, train_matrix, test_dataset, k))\n",
      "        print(\"Prepared score\")\n",
      "    return scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for factors in np.arange(20, 100, 20):\n",
      "    for regularization in np.linspace(0.1, 1, 5):\n",
      "        train_scores, test_scores = cross_validate_roc_auc(n_splits=3, factors=factors, regularization=regularization)\n",
      "        print(\"======================\")\n",
      "        print(\"{}, {}, {}, {}\".format(factors, regularization, np.mean(train_scores), np.mean(test_scores)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for factors in np.arange(200, 250, 50):\n",
      "    for regularization in np.linspace(5, 8, 5):\n",
      "        scores = cross_validate_precision_k(n_splits=3, k=10, factors=factors, regularization=regularization)\n",
      "        print(\"======================\")\n",
      "        print(\"{}, {}, {}\".format(factors, regularization, np.mean(scores)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def average_precision_k(dataset, result_matrix, mask, k):\n",
      "    result_matrix = result_matrix * mask\n",
      "    precisions = []\n",
      "    for voter in dataset[\"user_id\"].unique():\n",
      "        pred_items = np.argsort(-result_matrix[uid_to_idx[voter]])[0:k]\n",
      "        true_items = dataset[(dataset[\"user_id\"] == voter) & (dataset[\"like\"])][\"post_id\"]\n",
      "        right_items = [x for x in true_items if x in pred_items]\n",
      "        if (true_items.size == 0):\n",
      "            precisions.append(1)\n",
      "        else:\n",
      "            precisions.append(float(len(right_items) / min(k, true_items.size)))\n",
      "    return np.mean(precisions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def average_recall_k(dataset, result_matrix, mask, k):\n",
      "    result_matrix = result_matrix * mask\n",
      "    recalls = []\n",
      "    for voter in dataset[\"user_id\"].unique():\n",
      "        pred_items = np.argsort(-result_matrix[uid_to_idx[voter]])[0:k]\n",
      "        true_items = dataset[(dataset[\"user_id\"] == voter) & (dataset[\"like\"])][\"post_id\"]\n",
      "        right_items = [x for x in true_items if x in pred_items]\n",
      "        if (true_items.size == 0):\n",
      "            recalls.append(1)\n",
      "        else:\n",
      "            recalls.append(float(len(right_items) / true_items.size))\n",
      "    return np.mean(recalls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import ShuffleSplit\n",
      "from sklearn.metrics import roc_auc_score\n",
      "import implicit\n",
      "\n",
      "def cross_validate_precision_k(n_splits, **kwargs):\n",
      "    precisions = []\n",
      "    recalls = []\n",
      "    generator = ShuffleSplit(n_splits=n_splits)\n",
      "    for train, test in generator.split(dataset):\n",
      "        train_matrix = create_matrix(dataset, train, alpha=40).T\n",
      "        model = implicit.als.AlternatingLeastSquares(**kwargs)\n",
      "        model.fit(train_matrix)\n",
      "        result_matrix = np.dot(model.user_factors, model.item_factors.T)\n",
      "        mask = (1 - train_matrix.T.toarray())\n",
      "        precision = average_precision_k(dataset.iloc[test], result_matrix, mask, 10)\n",
      "        precisions.append(precision)\n",
      "        recall = average_recall_k(dataset.iloc[test], result_matrix, mask, 10)\n",
      "        recalls.append(recall)\n",
      "    return precisions, recalls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Save result to files for a server"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset_matrix = create_matrix(dataset, dataset.index).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.externals import joblib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joblib.dump(dataset_matrix.T, \"matrix.pkl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uid_to_idx = {}\n",
      "mid_to_idx = {}\n",
      "for (idx, uid) in enumerate(dataset[\"user_id\"].unique().tolist()):\n",
      "    uid_to_idx[uid] = idx\n",
      "    \n",
      "for (idx, mid) in enumerate(dataset[\"post_permlink\"].unique().tolist()):\n",
      "    mid_to_idx[mid] = idx\n",
      "    \n",
      "idx_to_uid = {v: k for k, v in uid_to_idx.items()}\n",
      "idx_to_mid = {v: k for k, v in mid_to_idx.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joblib.dump(uid_to_idx, \"uid_to_idx.pkl\")\n",
      "joblib.dump(idx_to_mid, \"idx_to_mid.pkl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import implicit\n",
      "model = implicit.als.AlternatingLeastSquares(factors=250, regularization=8)\n",
      "model.fit(dataset_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joblib.dump(model, \"model.pkl\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset.iloc[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create a new model with explicit and implicit features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implicit features and an error for each item"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import implicit\n",
      "dataset_matrix = create_matrix(dataset, dataset.index).T\n",
      "train, test = train_test_split(dataset, test_size=0.5) \n",
      "train_matrix = create_matrix(dataset, train.index).T\n",
      "model = implicit.als.AlternatingLeastSquares(factors=250, regularization=6)\n",
      "model.fit(train_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result_matrix = model.user_factors.dot(model.item_factors.T)\n",
      "delta_matrix = dataset_matrix - result_matrix.T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bins = np.linspace(-1, 1, 100)*10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "delta_matrix[0:10].reshape(-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "plt.hist(delta_matrix[0:10].reshape(-1), bins=bins)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implicit features - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Explicit features - TF-IDF, LDA topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load LDA model\n",
      "# Load dictionary\n",
      "# Get bow for each post\n",
      "# Get vector for each post"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts = pd.read_csv(\"posts.csv\").set_index(\"permlink\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset[\"post\"] = posts.loc[dataset[\"post_permlink\"]].reset_index()[\"text\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train, test = train_test_split(dataset, test_size=0.3)\n",
      "train_matrix = create_matrix(dataset, train.index).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import implicit\n",
      "implicit_model = implicit.als.AlternatingLeastSquares(factors=250, regularization=6)\n",
      "implicit_model.fit(train_matrix)\n",
      "dataset_users = pd.DataFrame([implicit_model.user_factors[uid_to_idx[x]] for x in dataset[\"user_id\"]])\n",
      "dataset_users.columns = [str(column) + '_user_factor' for column in dataset_users.columns]\n",
      "dataset_posts = pd.DataFrame([implicit_model.item_factors[mid_to_idx[x]] for x in dataset[\"post_permlink\"]])\n",
      "dataset_posts.columns = [str(column) + '_post_factor' for column in dataset_posts.columns]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora\n",
      "from gensim import models\n",
      "from nltk.tokenize import TweetTokenizer\n",
      "dictionary = corpora.Dictionary.load('./datasets/golos-corpora.dict')\n",
      "topic_model = models.ldamodel.LdaModel.load('./datasets/golos-corpora.lda_model', mmap='r')\n",
      "tokenizer = TweetTokenizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = topic_model.get_document_topics(dictionary.doc2bow(tokenizer.tokenize(str(dataset[\"post\"][109]))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizer.tokenize(str(dataset[\"post\"][109]).lower())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset_topics = [topic_model.get_document_topics(dictionary.doc2bow(tokenizer.tokenize(str(x).lower()))) for x in dataset[\"post\"]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectors_dataset_topics = []\n",
      "length = len(dataset_topics)\n",
      "for index, topic in enumerate(dataset_topics):\n",
      "    print(float(index *100 / length), end=\"\\r\")\n",
      "    data = [i for _, i in topic]\n",
      "    coordinates = ([i for i, _ in topic], np.zeros(len(topic)))\n",
      "    vector = sparse.coo_matrix((data, coordinates), shape=(100, 1)).toarray().reshape(-1)\n",
      "    vectors_dataset_topics.append(vector)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectors_dataset_topics = pd.DataFrame(vectors_dataset_topics)\n",
      "vectors_dataset_topics.columns = [str(column) + '_topic' for column in vectors_dataset_topics.columns]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_dataset = pd.concat([dataset, dataset_users, dataset_posts, vectors_dataset_topics], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_dataset[\"rating\"] = [np.dot(implicit_model.user_factors[uid_to_idx[row[\"user_id\"]]], implicit_model.item_factors[mid_to_idx[row[\"post_permlink\"]]]) for _, row in full_dataset.iterrows()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(full_dataset.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_dataset.drop(['level_0', 'index', 'Unnamed: 0', 'post'], axis=1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_dataset.to_csv(\"./full_dataset.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_X = full_dataset.loc[train.index].drop([\"like\", \"user_id\", \"post_permlink\"], axis=1)\n",
      "train_y = full_dataset.loc[train.index][\"like\"]\n",
      "\n",
      "test_X = full_dataset.loc[test.index].drop([\"like\", \"user_id\", \"post_permlink\"], axis=1)\n",
      "test_y = full_dataset.loc[test.index][\"like\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from xgboost import XGBClassifier\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "for i in range(10, 100, 20):\n",
      "    model = XGBClassifier(n_estimators=i).fit(train_X, train_y, verbose=True)\n",
      "    print(i)\n",
      "    print(roc_auc_score(train_y, model.predict(train_X)))\n",
      "    print(roc_auc_score(test_y, model.predict(test_X)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from xgboost import XGBClassifier\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "for i in np.linspace(0.01, 1, 10):\n",
      "    model = XGBClassifier(n_estimators=200, learning_rate=i).fit(train_X, train_y, verbose=True)\n",
      "    print(i)\n",
      "    print(roc_auc_score(train_y, model.predict(train_X)))\n",
      "    print(roc_auc_score(test_y, model.predict(test_X)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from xgboost import XGBClassifier\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "for i in np.linspace(0.01, 1, 10):\n",
      "    model = XGBClassifier(n_estimators=80, learning_rate=0.1, subsample=i).fit(train_X, train_y, verbose=True)\n",
      "    print(i)\n",
      "    print(roc_auc_score(train_y, model.predict(train_X)))\n",
      "    print(roc_auc_score(test_y, model.predict(test_X)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.utils import np_utils\n",
      "categorical_train_y = np_utils.to_categorical(train_y)\n",
      "categorical_test_y = np_utils.to_categorical(test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.models import Model\n",
      "from keras.layers import Input, Dense\n",
      "\n",
      "input_layer = Input(shape=(train_X.shape[1],))\n",
      "x = Dense(100)(input_layer)\n",
      "x = Dense(100)(x)\n",
      "x = Dense(2)(x)\n",
      "model = Model(inputs=input_layer, outputs=x)\n",
      "model.compile('adam', 'categorical_crossentropy', metrics=[\"accuracy\"])\n",
      "model.fit(train_X.as_matrix(), categorical_train_y, batch_size=64, epochs=100, validation_data=(test_X.as_matrix(), categorical_test_y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create model with fit and recommend method, use standard methods\n",
      "- A model predict rating for user i and pose j by a formula:\n",
      "$$ r_{ij} = \\alpha t_j + \\beta f_i + u_ip_j $$\n",
      "- Predict:\n",
      "    Predict all ratings for an user, then add some post-dependent rating to it\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "fLDA:\n",
      "$$r_ij = \\alpha u_i + \\beta p_j + s_i t_j$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u043f\u043e\u0441\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0440\u0430\u0432\u044f\u0442\u0441\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e\n",
      "2. \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438\u0445 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435\n",
      "3. \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_ffm_matrix(matrix):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from libffm_python import ffm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from annoy import AnnoyIndex\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(dataset, test_size=0.3)\n",
      "train_matrix = create_matrix(dataset, train.index, alpha=40).T\n",
      "annoy = AnnoyIndex(train_matrix.shape[1])\n",
      "print(train_matrix.shape[1])\n",
      "for index, row in enumerate(train_matrix.toarray()):\n",
      "    annoy.add_item(index, row.reshape(-1))\n",
      "annoy.build(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(dataset[(dataset[\"user_id\"] == dataset.iloc[0][\"user_id\"]) & (dataset[\"like\"] == 1)][[\"post_permlink\", \"like\"]],) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(dataset.loc[14][\"post_permlink\"])\n",
      "print(np.array([idx_to_mid[x] for x in annoy.get_nns_by_item(mid_to_idx[dataset.loc[75][\"post_permlink\"]], 20)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# LibFFM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "dataset = pd.read_csv(\"./events.csv\")\n",
      "dataset[\"like\"] = dataset[\"like\"] == 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# posts = pd.read_csv('posts.csv')\n",
      "posts = pd.read_csv('posts.csv')\n",
      "# posts = pd.concat([posts, posts_new], axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts = posts.set_index(\"post_permlink\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import sparse\n",
      "import numpy as np\n",
      "\n",
      "uid_to_idx = {}\n",
      "mid_to_idx = {}\n",
      "for (idx, uid) in enumerate(dataset[\"user_id\"].unique().tolist()):\n",
      "    uid_to_idx[uid] = idx\n",
      "    \n",
      "for (idx, mid) in enumerate(dataset[\"post_permlink\"].unique().tolist()):\n",
      "    mid_to_idx[mid] = idx\n",
      "    \n",
      "idx_to_uid = {v: k for k, v in uid_to_idx.items()}\n",
      "idx_to_mid = {v: k for k, v in mid_to_idx.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aid_to_idx = {}\n",
      "tid_to_idx = {}\n",
      "for (idx, aid) in enumerate(posts[\"author\"].unique().tolist()):\n",
      "    aid_to_idx[aid] = idx\n",
      "    \n",
      "for (idx, tid) in enumerate(posts[\"parent_permlink\"].unique().tolist()):\n",
      "    tid_to_idx[tid] = idx\n",
      "\n",
      "aid_to_idx[\"nan\"] = posts[\"author\"].unique().shape[0]\n",
      "tid_to_idx[\"nan\"] = posts[\"parent_permlink\"].unique().shape[0]\n",
      "    \n",
      "idx_to_aid = {v: k for k, v in aid_to_idx.items()}\n",
      "idx_to_tid = {v: k for k, v in tid_to_idx.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_ffm_dataset(dataset):\n",
      "    X = []\n",
      "    for index, row in dataset.iterrows():\n",
      "        author = \"nan\"\n",
      "        parent_permlink = \"nan\"\n",
      "        if row[\"post_permlink\"] in posts.index:\n",
      "            author = posts.loc[row[\"post_permlink\"]][\"author\"]\n",
      "            parent_permlink = posts.loc[row[\"post_permlink\"]][\"parent_permlink\"]\n",
      "        X.append([\n",
      "            (0, uid_to_idx[row[\"user_id\"]], 1), \n",
      "            (1, mid_to_idx[row[\"post_permlink\"]], 1), \n",
      "            (2, aid_to_idx[author], 1), \n",
      "            (3, tid_to_idx[parent_permlink], 1)\n",
      "        ])\n",
      "    return X, [int(y) for y in dataset[\"like\"]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train, test = train_test_split(dataset, test_size=0.3)\n",
      "train_X, train_y = create_ffm_dataset(train)\n",
      "test_X, test_y = create_ffm_dataset(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 223
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from libffm_python import ffm\n",
      "from sklearn.metrics import roc_auc_score\n",
      "\n",
      "train_ffm_data = ffm.FFMData(train_X, train_y)\n",
      "test_ffm_data = ffm.FFMData(test_X, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_iter = 100\n",
      "\n",
      "model = ffm.FFM(eta=0.05, lam=0.01, k=40)\n",
      "model.init_model(train_ffm_data)\n",
      "\n",
      "for i in range(n_iter):\n",
      "    print('iteration %d, ' % i, end='')\n",
      "    model.iteration(train_ffm_data)\n",
      "\n",
      "    y_pred = model.predict(train_ffm_data)\n",
      "    auc = roc_auc_score(train_y, y_pred)\n",
      "    print('train auc %.4f ' % auc, end='')\n",
      "    \n",
      "    y_pred = model.predict(test_ffm_data)\n",
      "    auc = roc_auc_score(test_y, y_pred)\n",
      "    print('test auc %.4f' % auc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0, train auc 0.7238 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.6928"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 1, train auc 0.7570 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7116"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 2, train auc 0.7790 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7214"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 3, train auc 0.7940 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7274"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 4, train auc 0.8082 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7322"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 5, train auc 0.8206 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7354"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 6, train auc 0.8327 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7394"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 7, train auc 0.8443 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7426"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 8, train auc 0.8560 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7432"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 9, train auc 0.8686 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7458"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 10, train auc 0.8811 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7481"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 11, train auc 0.8932 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7493"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 12, train auc 0.9046 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7506"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 13, train auc 0.9157 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7516"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 14, train auc 0.9248 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7515"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 15, train auc 0.9347 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7523"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 16, train auc 0.9421 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7522"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 17, train auc 0.9486 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7514"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 18, train auc 0.9547 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7506"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 19, train auc 0.9602 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7503"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 20, train auc 0.9650 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7493"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 21, train auc 0.9687 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7485"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 22, train auc 0.9719 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7474"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 23, train auc 0.9750 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7463"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 24, train auc 0.9771 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7451"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 25, train auc 0.9793 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7440"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 26, train auc 0.9814 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7426"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 27, train auc 0.9830 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7418"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 28, train auc 0.9846 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7405"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 29, train auc 0.9857 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7396"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 30, train auc 0.9868 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7384"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 31, train auc 0.9878 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test auc 0.7375"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 32, "
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-236-d4b3db0af549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ffm_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train auc %.4f '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/earth/.local/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    263\u001b[0m     return _average_binary_score(\n\u001b[1;32m    264\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/earth/.local/lib/python3.5/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     68\u001b[0m                          ''.format(average_options))\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/earth/.local/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'continuous'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'multiclass'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m  \u001b[0;31m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/earth/.local/lib/python3.5/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/earth/.local/lib/python3.5/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 236
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = create_ffm_dataset(dataset)\n",
      "ffm_data = ffm.FFMData(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_iter = 8\n",
      "\n",
      "model = ffm.FFM(eta=0.1, lam=0.01, k=100)\n",
      "model.init_model(ffm_data)\n",
      "\n",
      "for i in range(n_iter):\n",
      "    print('iteration %d, ' % i, end='')\n",
      "    model.iteration(ffm_data)\n",
      "    y_pred = model.predict(ffm_data)\n",
      "    auc = roc_auc_score(y, y_pred)\n",
      "    print('train auc %.4f ' % auc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_users = dataset[\"user_id\"].unique()\n",
      "for user in unique_users:\n",
      "    if dataset[(dataset[\"user_id\"] == user) & (dataset[\"like\"] == 1) & (dataset[\"post_permlink\"].apply(lambda x: x in posts.index))].shape[0] > 10:\n",
      "        break;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-12-9126d2edeec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0munique_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_users\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"like\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"post_permlink\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/_libs/lib.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m<ipython-input-12-9126d2edeec3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0munique_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_users\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"like\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"post_permlink\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_title(post):\n",
      "    if post not in posts.index:\n",
      "        return \"nan\"\n",
      "    else:\n",
      "        return posts.loc[post][\"title\"]\n",
      "    \n",
      "def get_titles(posts):\n",
      "    return [get_title(post) for post in posts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(get_titles(dataset[(dataset[\"user_id\"] == user) & (dataset[\"like\"] == 1)][\"post_permlink\"].as_matrix()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_dataset = pd.DataFrame()\n",
      "test_dataset[\"post_permlink\"] = dataset[\"post_permlink\"].unique()\n",
      "test_dataset[\"user_id\"] = user\n",
      "test_dataset[\"like\"] = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ffm_data = ffm.FFMData(*create_ffm_dataset(test_dataset))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prediction = model.predict(ffm_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for post in np.argsort(-prediction)[0:30]:\n",
      "    print(\"{} = {}\".format(get_title(test_dataset[\"post_permlink\"].iloc[post]), prediction[post]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}