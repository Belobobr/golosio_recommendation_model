{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.engine import Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import keyedvectors\n",
    "from collections import defaultdict\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "nb_filters = 1200  # number of filters\n",
    "n_gram = 3  # n-gram, or window size of CNN/ConvNet\n",
    "maxlen = 15  # maximum number of words in a sentence\n",
    "vecsize = 300  # length of the embedded vectors in the model \n",
    "cnn_dropout = 0.0  # dropout rate for CNN/ConvNet\n",
    "final_activation = 'softmax'  # activation function. Options: softplus, softsign, relu, tanh, sigmoid, hard_sigmoid, linear.\n",
    "dense_wl2reg = 0.0  # dense_wl2reg: L2 regularization coefficient\n",
    "dense_bl2reg = 0.0  # dense_bl2reg: L2 regularization coefficient for bias\n",
    "optimizer = 'adam'  # optimizer for gradient descent. Options: sgd, rmsprop, adagrad, adadelta, adam, adamax, nadam\n",
    "\n",
    "# utility functions\n",
    "\n",
    "def retrieve_csvdata_as_dict(filepath):\n",
    "    \"\"\"\n",
    "    Retrieve the training data in a CSV file, with the first column being the\n",
    "    class labels, and second column the text data. It returns a dictionary with\n",
    "    the class labels as keys, and a list of short texts as the value for each key.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    category_col, descp_col = df.columns.values.tolist()\n",
    "    shorttextdict = dict()\n",
    "    for category, descp in zip(df[category_col], df[descp_col]):\n",
    "        if type(descp) == str:\n",
    "            shorttextdict.setdefault(category, []).append(descp)\n",
    "    return shorttextdict\n",
    "\n",
    "def subjectkeywords():\n",
    "    \"\"\"\n",
    "    Return an example data set, with three subjects and corresponding keywords.\n",
    "    This is in the format of the training input.\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(os.getcwd(), 'datasets/keras_classifier_training_data.csv')\n",
    "    return retrieve_csvdata_as_dict(data_path)\n",
    "\n",
    "def convert_trainingdata(classdict):\n",
    "    \"\"\"\n",
    "    Convert the training data into format put into the neural networks.\n",
    "    \"\"\"\n",
    "    classlabels = classdict.keys()\n",
    "    lblidx_dict = dict(zip(classlabels, range(len(classlabels))))\n",
    "\n",
    "    # tokenize the words, and determine the word length\n",
    "    phrases = []\n",
    "    indices = []\n",
    "    for label in classlabels:\n",
    "        for shorttext in classdict[label]:\n",
    "            shorttext = shorttext if type(shorttext) == str else ''\n",
    "            category_bucket = [0]*len(classlabels)\n",
    "            category_bucket[lblidx_dict[label]] = 1\n",
    "            indices.append(category_bucket)\n",
    "            phrases.append(shorttext)\n",
    "\n",
    "    return classlabels, phrases, indices\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\" \n",
    "    Process the input text by tokenizing and padding it.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    x_train = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model_wv = keyedvectors.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# The dataset 'GoogleNews-vectors-negative300.bin.gz' can be downloaded from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "w2v_model_wv = keyedvectors.KeyedVectors.load_word2vec_format('datasets/ruscorpora_1_300_10.bin', binary=True)\n",
    "# w2v_model_wv = keyedvectors.KeyedVectors.load_word2vec_format('datasets/wiki.ru.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Vocab at 0x12c20f668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_wv.vocab['топология_NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mathematics': ['алгебра_NOUN', 'топология_NOUN', 'вычисление_NOUN', 'исчисление_NOUN', 'статистика_NOUN', 'вероятность_NOUN'], 'physics': ['электродинамика_NOUN', 'электрон_NOUN'], 'politics': ['президент_NOUN', 'бойкот_NOUN', 'референдум_NOUN'], 'biology': ['организм_NOUN', 'мутация_NOUN', 'растение_NOUN', 'жизнь_NOUN']}\n"
     ]
    }
   ],
   "source": [
    "trainclassdict = subjectkeywords()\n",
    "\n",
    "nb_labels = len(trainclassdict)  # number of class labels\n",
    "print(trainclassdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding layer corresponding to our trained Word2Vec model\n",
    "embedding_layer = w2v_model_wv.get_embedding_layer()\n",
    "\n",
    "# create a convnet to solve our classification task\n",
    "sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(filters=nb_filters, kernel_size=n_gram, padding='valid', activation='relu', input_shape=(maxlen, vecsize))(embedded_sequences)\n",
    "x = MaxPooling1D(pool_size=maxlen - n_gram + 1)(x)\n",
    "x = Flatten()(x)\n",
    "preds = Dense(nb_labels, activation=final_activation, kernel_regularizer=l2(dense_wl2reg), bias_regularizer=l2(dense_bl2reg))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 0s - loss: 1.3943 - acc: 0.0667\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s - loss: 1.4734 - acc: 0.4000\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s - loss: 1.3933 - acc: 0.2667\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s - loss: 1.1189 - acc: 0.4000\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s - loss: 1.0069 - acc: 0.6667\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s - loss: 0.9366 - acc: 0.7333\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s - loss: 0.8718 - acc: 0.8000\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s - loss: 0.8133 - acc: 0.8667\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s - loss: 0.7616 - acc: 0.8667\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s - loss: 0.7152 - acc: 0.8667\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s - loss: 0.6693 - acc: 0.9333\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s - loss: 0.6242 - acc: 0.9333\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s - loss: 0.5782 - acc: 1.0000\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s - loss: 0.5373 - acc: 0.9333\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s - loss: 0.4949 - acc: 1.0000\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s - loss: 0.4661 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s - loss: 0.4386 - acc: 1.0000\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s - loss: 0.4119 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s - loss: 0.3816 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s - loss: 0.3439 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s - loss: 0.3083 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s - loss: 0.2755 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s - loss: 0.2498 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s - loss: 0.2278 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s - loss: 0.2085 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s - loss: 0.1918 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s - loss: 0.1766 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s - loss: 0.1633 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s - loss: 0.1507 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s - loss: 0.1399 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s - loss: 0.1293 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s - loss: 0.1201 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s - loss: 0.1107 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s - loss: 0.1026 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s - loss: 0.0948 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s - loss: 0.0878 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s - loss: 0.0811 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s - loss: 0.0751 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s - loss: 0.0693 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s - loss: 0.0642 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s - loss: 0.0594 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s - loss: 0.0553 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s - loss: 0.0512 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s - loss: 0.0476 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s - loss: 0.0442 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s - loss: 0.0412 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s - loss: 0.0383 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s - loss: 0.0357 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s - loss: 0.0333 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s - loss: 0.0311 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "classlabels, x_train, y_train = convert_trainingdata(trainclassdict)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "fit_ret_val = model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mathematics': 0.065678559, 'physics': 0.79528475, 'politics': 0.064391196, 'biology': 0.074645475}\n"
     ]
    }
   ],
   "source": [
    "input_text = 'самый_DET интересный_ADJ вопрос_NOUN который_DET задавать_VERB ученый_NOUN исследователь_NOUN старение_NOUN сам_DET принимать_VERB увеличение_NOUN продолжительность_NOUN жизнь_NOUN узнавать_VERB это_PRON большой_ADJ конференция_NOUN старение_NOUN долголетие_NOUN сразу_ADV видно_ADV современный_ADJ состояние_NOUN дело_NOUN наука_NOUN геропротектор_NOUN услышать_VERB довольно_ADV разный_ADJ ответ_NOUN'\n",
    "\n",
    "matrix = process_text(input_text)\n",
    "\n",
    "predictions = model.predict(matrix)\n",
    "\n",
    "# get the actual categories from output\n",
    "scoredict = {}\n",
    "for idx, classlabel in zip(range(len(classlabels)), classlabels):\n",
    "    scoredict[classlabel] = predictions[0][idx]\n",
    "\n",
    "print(scoredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
