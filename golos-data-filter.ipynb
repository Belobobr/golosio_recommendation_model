{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from lxml import etree\n",
    "from lxml.html.clean import Cleaner\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = codecs.open('datasets/golos_io_topics.xml', encoding='utf-8')\n",
    "data = file.read();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = Cleaner()\n",
    "cleaner.allow_tags = ['item', 'field']\n",
    "cleaner.remove_unknown_tags = False\n",
    "clean_data = cleaner.clean_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res.xml', encoding='utf-8', mode='w+')\n",
    "fout.write(clean_data)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем список стоп-слов и добавляем свои\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('russian')\n",
    "more_stopwords = 'https, http, без, более, бы, был, была, были, было, быть, вам, вас, ведь, весь, вдоль, вместо, вне, вниз, внизу, внутри, во, вокруг, вот, все, всегда, всего, всех, вы, где, да, давай, давать, даже, для, до, достаточно, его, ее, её, если, есть, ещё, же, за, здесь, из, из-за, или, им, иметь, их, как, как-то, кто, когда, кроме, кто, ли, либо, мне, может, мои, мой, мы, на, навсегда, над, надо, наш, не, него, неё, нет, ни, них, но, ну, об, однако, он, она, они, оно, от, отчего, очень, по, под, после, потому, потому что, почти, при, про, снова, со, так, также, такие, такой, там, те, тем, то, того, тоже, той, только, том, тут, ты, уже, хотя, чего, чего-то, чей, чем, что, чтобы, чьё, чья, эта, эти, это'\n",
    "more_stopwords_list = more_stopwords.split(\", \")\n",
    "stopwords_list += more_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.fromstring(clean_data)\n",
    "items = tree.xpath(\"//field[@name='body']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list = [item.text for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res2.txt', encoding='utf-8', mode='w+')\n",
    "for item in seq_list:\n",
    "    print(item, file=fout, sep=\"\\n\")\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "def convert_prefix2utags(mystem_prefix):\n",
    "    utags_dict = {'_A':       '_ADJ',\n",
    "                  '_ADV':     '_ADV',                                                                                                                                                                                                                                                                    \n",
    "                  '_ADVPRO':  '_ADV',                                                                                                                                                                                                                                                                    \n",
    "                  '_ANUM':    '_ADJ',                                                                                                                                                                                                                                                                 \n",
    "                  '_APRO':    '_DET',                                                                                                                                                                                                                                                                   \n",
    "                  '_COM':     '_ADJ',                                                                                                                                                                                                                                                                    \n",
    "                  '_CONJ':    '_SCONJ',                                                                                                                                                                                                                                                                  \n",
    "                  '_INTJ':    '_INTJ',                                                                                                                                                                                                                                                                   \n",
    "                  '_NONLEX':  '_X',                                                                                                                                                                                                                                                                      \n",
    "                  '_NUM':     '_NUM',                                                                                                                                                                                                                                                                    \n",
    "                  '_PART':    '_PART',                                                                                                                                                                                                                                                                   \n",
    "                  '_PR':      '_ADP',                                                                                                                                                                                                                                                                    \n",
    "                  '_S':       '_NOUN',                                                                                                                                                                                                                                                                   \n",
    "                  '_SPRO':    '_PRON',                                                                                                                                                                                                                                                                   \n",
    "                  '_UNKN':    '_X',                                                                                                                                                                                                                                                                      \n",
    "                  '_V':       '_VERB',\n",
    "                  '':        ''}\n",
    "    return utags_dict[mystem_prefix]\n",
    "\n",
    "def get_word_prefix(word):\n",
    "    lemmas = mystem.analyze(word)\n",
    "    prefix = \"\"\n",
    "    if (isinstance(lemmas, collections.Iterable)) and ('analysis' in lemmas[0]):\n",
    "        try:\n",
    "            prefix = \"_\"+(lemmas[0]['analysis'][0]['gr'].split(\"=\")[0].split(\",\")[0]) \n",
    "        except IndexError:\n",
    "            prefix = \"\"\n",
    "    return convert_prefix2utags(prefix)\n",
    "\n",
    "def lemmatize_words(word_list):\n",
    "    processed_word_list = []\n",
    "    for word in word_list:\n",
    "        word = mystem.lemmatize(word)[0]\n",
    "# remove lemma prefixes        \n",
    "#        word_w_prefix = word+get_word_prefix(word)\n",
    "        processed_word_list.append(word)\n",
    "    return processed_word_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['человек']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_words([\"люди\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_list):\n",
    "        processed_word_list = []\n",
    "        \n",
    "        \n",
    "        for word in word_list:\n",
    "            word = word.lower() # in case they arenet all lower cased\n",
    "            if word not in stopwords_list and len(word) > 2:\n",
    "                processed_word_list.append(word)\n",
    "        return processed_word_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = nltk.word_tokenize(seq_list[1100])\n",
    "test_clean = remove_stopwords(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//s15.postimg.org/dv7fam3ln/a2110ac82294.jpg', 'всем', 'привет', 'недавно', 'одна', 'девочка', 'звала', 'crp.center', 'освободилась', '1200', 'долларов', 'думаю', 'эххх', '...', 'вложила', 'криптовалюту', 'верила', 'проект', 'знаю', 'почему', 'слава', 'богу', 'интуиция', 'подвела', 'говорит', '1000', 'долларов', 'сняла', 'туда', '000', 'слава', 'богу', 'представьте', 'сколько', 'людей', 'потеряло', 'печально', 'видео', 'обращение', 'виталий', 'говорит', 'будут', 'работать', 'обычно', 'сказки', 'вкладывайте', '//www.youtube.com/watch', 'time_continue=393', 'v=g9v3ix73moy', 'всем', 'мира', 'стабильного', 'дохода']\n"
     ]
    }
   ],
   "source": [
    "print(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc_text):\n",
    "    words_list = remove_stopwords(nltk.word_tokenize(doc_text))\n",
    "# пока без леммизации\n",
    "    # lemmas_list = lemmatize_words(words_list)\n",
    "    lemmas_list = words_list\n",
    "    return \" \".join(lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lemma_doc(doc_text):\n",
    "    words_list = remove_stopwords(nltk.word_tokenize(doc_text))\n",
    "    lemmas_list = lemmatize_words(words_list)\n",
    "    return \" \".join(lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'//s15.postimg.org/dv7fam3ln/a2110ac82294.jpg всем привет недавно одна девочка звала crp.center освободилась 1200 долларов думаю эххх ... вложила криптовалюту верила проект знаю почему слава богу интуиция подвела говорит 1000 долларов сняла туда 000 слава богу представьте сколько людей потеряло печально видео обращение виталий говорит будут работать обычно сказки вкладывайте //www.youtube.com/watch time_continue=393 v=g9v3ix73moy всем мира стабильного дохода'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_doc = preprocess_doc(seq_list[1100])\n",
    "lemmas_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('говорит', 2),\n",
       " ('богу', 2),\n",
       " ('слава', 2),\n",
       " ('долларов', 2),\n",
       " ('всем', 2),\n",
       " ('дохода', 1),\n",
       " ('1000', 1),\n",
       " ('//s15.postimg.org/dv7fam3ln/a2110ac82294.jpg', 1),\n",
       " ('почему', 1),\n",
       " ('звала', 1),\n",
       " ('обращение', 1),\n",
       " ('будут', 1),\n",
       " ('представьте', 1),\n",
       " ('000', 1),\n",
       " ('...', 1),\n",
       " ('знаю', 1),\n",
       " ('освободилась', 1),\n",
       " ('сколько', 1),\n",
       " ('девочка', 1),\n",
       " ('crp.center', 1),\n",
       " ('мира', 1),\n",
       " ('интуиция', 1),\n",
       " ('вкладывайте', 1),\n",
       " ('вложила', 1),\n",
       " ('сняла', 1),\n",
       " ('подвела', 1),\n",
       " ('1200', 1),\n",
       " ('недавно', 1),\n",
       " ('проект', 1),\n",
       " ('печально', 1),\n",
       " ('одна', 1),\n",
       " ('time_continue=393', 1),\n",
       " ('//www.youtube.com/watch', 1),\n",
       " ('v=g9v3ix73moy', 1),\n",
       " ('видео', 1),\n",
       " ('верила', 1),\n",
       " ('сказки', 1),\n",
       " ('виталий', 1),\n",
       " ('криптовалюту', 1),\n",
       " ('думаю', 1),\n",
       " ('стабильного', 1),\n",
       " ('работать', 1),\n",
       " ('эххх', 1),\n",
       " ('потеряло', 1),\n",
       " ('привет', 1),\n",
       " ('людей', 1),\n",
       " ('туда', 1),\n",
       " ('обычно', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fd = FreqDist(nltk.word_tokenize(lemmas_doc))\n",
    "fd.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res-lines.txt', encoding='utf-8', mode='w+')\n",
    "for item in seq_list:\n",
    "    if item is not None:\n",
    "        processed_doc = preprocess_doc(item)\n",
    "        print(processed_doc, file=fout, sep=\"\\n\")\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res-lemmas.txt', encoding='utf-8', mode='w+')\n",
    "for item in seq_list:\n",
    "    if item is not None:\n",
    "        processed_lemma_doc = preprocess_lemma_doc(item)\n",
    "        print(processed_lemma_doc, file=fout, sep=\"\\n\")\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"люди употребляют различные препараты делятся своими впечатлениями . акцент ноотропы , продление жизни . всему этому процессу хватает организованности . собственно этим займемся . прекрасный план . шаг 1. личный медицинский кабинет . должны результаты ваших анализов . сдавали . терапии , которые осуществляли . самое важно , должны результаты диагностики старения . плюс , , описание интервенций против старения . биомаркеры скорости старения далеки совершенств\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-80ffdd9fda40>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-80ffdd9fda40>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    (/, print(lemmas))\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "lemmas = m.analyze(text)\n",
    "// print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas[0]['analysis'][0]['gr'].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
