{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from lxml import etree\n",
    "from lxml.html.clean import Cleaner\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = codecs.open('datasets/golos_io_topics.xml', encoding='utf-8')\n",
    "data = file.read();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = Cleaner()\n",
    "cleaner.allow_tags = ['item', 'field']\n",
    "cleaner.remove_unknown_tags = False\n",
    "clean_data = cleaner.clean_html(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res.xml', encoding='utf-8', mode='w+')\n",
    "fout.write(clean_data)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.fromstring(clean_data)\n",
    "items = tree.xpath(\"//field[@name='body']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_list = [item.text for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res2.txt', encoding='utf-8', mode='w+')\n",
    "for item in seq_list:\n",
    "    print(item, file=fout, sep=\"\\n\")\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "def convert_prefix2utags(mystem_prefix):\n",
    "    utags_dict = {'_A':       '_ADJ',\n",
    "                  '_ADV':     '_ADV',                                                                                                                                                                                                                                                                    \n",
    "                  '_ADVPRO':  '_ADV',                                                                                                                                                                                                                                                                    \n",
    "                  '_ANUM':    '_ADJ',                                                                                                                                                                                                                                                                 \n",
    "                  '_APRO':    '_DET',                                                                                                                                                                                                                                                                   \n",
    "                  '_COM':     '_ADJ',                                                                                                                                                                                                                                                                    \n",
    "                  '_CONJ':    '_SCONJ',                                                                                                                                                                                                                                                                  \n",
    "                  '_INTJ':    '_INTJ',                                                                                                                                                                                                                                                                   \n",
    "                  '_NONLEX':  '_X',                                                                                                                                                                                                                                                                      \n",
    "                  '_NUM':     '_NUM',                                                                                                                                                                                                                                                                    \n",
    "                  '_PART':    '_PART',                                                                                                                                                                                                                                                                   \n",
    "                  '_PR':      '_ADP',                                                                                                                                                                                                                                                                    \n",
    "                  '_S':       '_NOUN',                                                                                                                                                                                                                                                                   \n",
    "                  '_SPRO':    '_PRON',                                                                                                                                                                                                                                                                   \n",
    "                  '_UNKN':    '_X',                                                                                                                                                                                                                                                                      \n",
    "                  '_V':       '_VERB',\n",
    "                  '':        ''}\n",
    "    return utags_dict[mystem_prefix]\n",
    "\n",
    "def get_word_prefix(word):\n",
    "    lemmas = mystem.analyze(word)\n",
    "    prefix = \"\"\n",
    "    if (isinstance(lemmas, collections.Iterable)) and ('analysis' in lemmas[0]):\n",
    "        try:\n",
    "            prefix = \"_\"+(lemmas[0]['analysis'][0]['gr'].split(\"=\")[0].split(\",\")[0]) \n",
    "        except IndexError:\n",
    "            prefix = \"\"\n",
    "    return convert_prefix2utags(prefix)\n",
    "\n",
    "def lemmatize_words(word_list):\n",
    "    processed_word_list = []\n",
    "    for word in word_list:\n",
    "        word = mystem.lemmatize(word)[0]\n",
    "# remove lemma prefixes        \n",
    "#        word_w_prefix = word+get_word_prefix(word)\n",
    "        processed_word_list.append(word)\n",
    "    return processed_word_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['человек']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_words([\"люди\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_list):\n",
    "        processed_word_list = []\n",
    "        for word in word_list:\n",
    "            word = word.lower() # in case they arenet all lower cased\n",
    "            if word not in stopwords.words(\"russian\") and len(word) > 2:\n",
    "                processed_word_list.append(word)\n",
    "        return processed_word_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print (stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = nltk.word_tokenize(seq_list[1100])\n",
    "test_clean = remove_stopwords(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https', '//s15.postimg.org/dv7fam3ln/a2110ac82294.jpg', 'всем', 'привет', 'недавно', 'одна', 'девочка', 'звала', 'crp.center', 'освободилась', '1200', 'долларов', 'думаю', 'эххх', '...', 'вложила', 'криптовалюту', 'верила', 'проект', 'знаю', 'почему', 'слава', 'богу', 'интуиция', 'подвела', 'хотя', 'говорит', '1000', 'долларов', 'сняла', 'туда', '000', 'слава', 'богу', 'представьте', 'сколько', 'людей', 'потеряло', 'это', 'печально', 'видео', 'обращение', 'виталий', 'говорит', 'снова', 'будут', 'работать', 'это', 'обычно', 'сказки', 'вкладывайте', 'https', '//www.youtube.com/watch', 'time_continue=393', 'v=g9v3ix73moy', 'всем', 'мира', 'стабильного', 'дохода']\n"
     ]
    }
   ],
   "source": [
    "print(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc_text):\n",
    "    words_list = remove_stopwords(nltk.word_tokenize(doc_text))\n",
    "# пока без леммизации\n",
    "    // lemmas_list = lemmatize_words(words_list)\n",
    "    lemmas_list = words_list\n",
    "    return \" \".join(lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lemma_doc(doc_text):\n",
    "    words_list = remove_stopwords(nltk.word_tokenize(doc_text))\n",
    "    lemmas_list = lemmatize_words(words_list)\n",
    "    return \" \".join(lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https //s15.postimg.org/dv7fam3ln/a2110ac82294.jpg всем привет недавно одна девочка звала crp.center освободилась 1200 долларов думаю эххх ... вложила криптовалюту верила проект знаю почему слава богу интуиция подвела хотя говорит 1000 долларов сняла туда 000 слава богу представьте сколько людей потеряло это печально видео обращение виталий говорит снова будут работать это обычно сказки вкладывайте https //www.youtube.com/watch time_continue=393 v=g9v3ix73moy всем мира стабильного дохода'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_doc = preprocess_doc(seq_list[1100])\n",
    "lemmas_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('старения', 10),\n",
       " ('шаг', 8),\n",
       " ('исследования', 5),\n",
       " ('против', 5),\n",
       " ('жизни', 4),\n",
       " ('клинические', 4),\n",
       " ('всем', 4),\n",
       " ('терапий', 4),\n",
       " ('вопрос', 3),\n",
       " ('сами', 3),\n",
       " ('это', 3),\n",
       " ('витамин', 3),\n",
       " ('терапии', 3),\n",
       " ('типа', 3),\n",
       " ('открыть', 3),\n",
       " ('всё', 3),\n",
       " ('исследований', 3),\n",
       " ('проекта', 3),\n",
       " ('людей', 3),\n",
       " ('записи', 3),\n",
       " ('который', 2),\n",
       " ('далее', 2),\n",
       " ('какие', 2),\n",
       " ('работает', 2),\n",
       " ('научной', 2),\n",
       " ('одно', 2),\n",
       " ('принимать', 2),\n",
       " ('должны', 2),\n",
       " ('результаты', 2),\n",
       " ('анализов', 2),\n",
       " ('которые', 2),\n",
       " ('плюс', 2),\n",
       " ('нужно', 2),\n",
       " ('следующие', 2),\n",
       " ('желающим', 2),\n",
       " ('самому', 2),\n",
       " ('лично', 2),\n",
       " ('кто-то', 2),\n",
       " ('организация', 2),\n",
       " ('клинических', 2),\n",
       " ('краудфандинг', 2),\n",
       " ('новых', 2),\n",
       " ('создадим', 2),\n",
       " ('принадлежать', 2),\n",
       " ('участникам', 2),\n",
       " ('человек', 2),\n",
       " ('голос', 2),\n",
       " ('коварный', 2),\n",
       " ('пока', 2),\n",
       " ('ради', 2)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fd = FreqDist(nltk.word_tokenize(lemmas_doc))\n",
    "fd.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res-lines.txt', encoding='utf-8', mode='w+')\n",
    "for item in seq_list:\n",
    "    if item is not None:\n",
    "        processed_doc = preprocess_doc(item)\n",
    "        print(processed_doc, file=fout, sep=\"\\n\")\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = codecs.open('datasets/golos_io_topics-res-lemmas.txt', encoding='utf-8', mode='w+')\n",
    "for item in seq_list:\n",
    "    if item is not None:\n",
    "        processed_lemma_doc = preprocess_lemma_doc(item)\n",
    "        print(processed_lemma_doc, file=fout, sep=\"\\n\")\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"люди употребляют различные препараты делятся своими впечатлениями . акцент ноотропы , продление жизни . всему этому процессу хватает организованности . собственно этим займемся . прекрасный план . шаг 1. личный медицинский кабинет . должны результаты ваших анализов . сдавали . терапии , которые осуществляли . самое важно , должны результаты диагностики старения . плюс , , описание интервенций против старения . биомаркеры скорости старения далеки совершенств\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-80ffdd9fda40>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-80ffdd9fda40>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    (/, print(lemmas))\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "lemmas = m.analyze(text)\n",
    "// print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas[0]['analysis'][0]['gr'].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
